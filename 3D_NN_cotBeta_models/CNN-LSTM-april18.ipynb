{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f683539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, TimeDistributed, LSTM, Conv3D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import CSVLogger\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import copy\n",
    "\n",
    "BATCH_SIZE = 2 \n",
    "EPOCHS = 1\n",
    "NUMBER_TEST_SET = 10 #will do train/test split before this, and pass already-made sets in\n",
    "TEMPORAL_LENGTH = 8 #use first 8 frames (these are 16 frames in each video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed62e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_generator(data_path,data_files,temporal_stride=1,temporal_length=TEMPORAL_LENGTH):\n",
    "    print(data_files)\n",
    "    for f in data_files: \n",
    "        tmp_df = pd.read_csv(os.path.join(data_path,f), sep=',')\n",
    "        label_list = list(tmp_df['label'])  \n",
    "        total_images = len(label_list) \n",
    "        if total_images==temporal_length: \n",
    "            num_samples = int((total_images-temporal_length)/temporal_stride)+1\n",
    "            print ('num of frames: {}: {}'.format(f,num_samples))\n",
    "            img_list = list(tmp_df['FileName'])\n",
    "        else: # if the number of frames < than temporal length , discard it\n",
    "            print ('num of frames is less than temporal length; hence discarding this file-{}'.format(f))\n",
    "            continue\n",
    "\n",
    "        start_frame = 0\n",
    "        samples = deque()  \n",
    "        samp_count=0  \n",
    "        for img in img_list:\n",
    "            samples.append(img)\n",
    "            if len(samples)==temporal_length: \n",
    "                samples_c=copy.deepcopy(samples) \n",
    "                samp_count+=1\n",
    "                for t in range(temporal_stride): \n",
    "                    samples.popleft()\n",
    "                yield samples_c,label_list[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples(data_cat='train',temporal_stride=1,temporal_length=TEMPORAL_LENGTH):\n",
    "    data_path = os.path.join('data_files',data_cat)\n",
    "    data_files = os.listdir(data_path)\n",
    "    #generator to read the samples\n",
    "    file_gen = file_generator(data_path,data_files,temporal_stride,temporal_length)\n",
    "    iterator = True\n",
    "    data_list = []\n",
    "    while iterator:\n",
    "        try:\n",
    "            x,y = next(file_gen)\n",
    "            x=list(x)\n",
    "            data_list.append([x,y])\n",
    "        except Exception as e:\n",
    "            print ('Exception: ',e)\n",
    "            iterator = False\n",
    "            print ('end of data generator')\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c3c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_samples(data_cat='train',temporal_stride=1,temporal_length=TEMPORAL_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9f6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_samples(data_cat='test',temporal_stride=1,temporal_length=TEMPORAL_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc52f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Total number of train samples:',len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed09b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08067eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Total number of test samples:',len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c91fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(samples):\n",
    "    data = shuffle(samples,random_state=2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0142dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data,batch_size=BATCH_SIZE,temporal_padding='same',shuffle=True):               \n",
    "    num_samples = len(data)\n",
    "    if shuffle:\n",
    "        data = shuffle_data(data)\n",
    "    while True:   \n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            print ('starting index: ', offset) \n",
    "            batch_samples = data[offset:offset+batch_size]\n",
    "            \n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            \n",
    "            for batch_sample in batch_samples: \n",
    "                print(batch_sample)\n",
    "                # Load image (X)\n",
    "                x = batch_sample[0] #image\n",
    "                y = batch_sample[1] #label\n",
    "                temp_data_list = []\n",
    "                for img in x:\n",
    "                    try:\n",
    "                        img = np.load(img)                        \n",
    "                        temp_data_list.append(img)\n",
    "                    except Exception as e:\n",
    "                        print (e)\n",
    "                        print ('error reading in frame: ',img)                      \n",
    "\n",
    "                X_train.append(temp_data_list)\n",
    "                y_train.append(y)\n",
    "\n",
    "            X_train = np.array(X_train)   \n",
    "            y_train = np.array(y_train)\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "        \n",
    "            print(X_train.shape)               \n",
    "            yield X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea2200d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "train_generator = data_generator(train_data,batch_size=BATCH_SIZE,shuffle=True)\n",
    "\n",
    "test_generator = data_generator(test_data,batch_size=1,shuffle=False) \n",
    "#x,y = next(train_generator)\n",
    "#xx,yy = next(train_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f849eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "    TimeDistributed(\n",
    "        Conv2D(64, (3,3), activation='relu'), \n",
    "        input_shape=(8, 13, 21, 1))\n",
    "    )\n",
    "    model.add(\n",
    "    TimeDistributed(\n",
    "        Conv2D(64, (3,3), activation='relu')\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "    TimeDistributed(\n",
    "        Flatten()\n",
    "        )\n",
    "    )        \n",
    "    model.add(LSTM(1024, activation='relu', return_sequences=False))\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    #model.add(Dropout(.01))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='Huber', optimizer=Adam(), metrics=['mean_squared_error'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "model = get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"cp.ckpt\"\n",
    "earlyStop_callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=False,\n",
    "                                                 verbose=1)\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b9ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(train_generator,\n",
    "                 steps_per_epoch=(len(train_data)/BATCH_SIZE),\n",
    "                 epochs=EPOCHS,\n",
    "                 validation_data=(test_generator), validation_steps=(len(test_data)/BATCH_SIZE),\n",
    "                 use_multiprocessing=True,workers=6,\n",
    "                 callbacks=[cp_callback, csv_logger, earlyStop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79accbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = model.predict_generator(test_generator, len(test_data)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_steps=NUMBER_TEST_SET \n",
    "truth = []\n",
    "pred = []\n",
    "\n",
    "for i in range(test_steps):\n",
    "    x, y = next(test_generator)\n",
    "    pred.append(model.predict(x))\n",
    "    truth.append(y) \n",
    "\n",
    "pred = np.concatenate(pred)\n",
    "truth = np.concatenate(truth)\n",
    "print(len(truth))\n",
    "print(len(pred))\n",
    "\n",
    "df_predict = pd.DataFrame(pred, columns=['cotBeta'])\n",
    "df_true = pd.DataFrame(truth, columns=['cotBeta'])\n",
    "df_predict.to_csv('predictions.csv')\n",
    "df_true.to_csv('trueLabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_true['cotBeta']-df_predict['cotBeta'], kde=False, bins=50)\n",
    "plt.xlabel('cotBeta residual')\n",
    "plt.ylabel('frequency')\n",
    "plt.xlim([-.3,.3])\n",
    "plt.title('Cot Beta Residual')\n",
    "plt.savefig('cotBeta-resolution.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
